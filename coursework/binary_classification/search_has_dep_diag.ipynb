{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "# How are we going to use evaluate the performance? \n",
    "# 1. accuracy\n",
    "from sklearn import metrics\n",
    "# 2. f1 score \n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Machine learning models \n",
    "\n",
    "# Linear Regression \n",
    "# url : https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# SVM\n",
    "# url: https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "from sklearn import svm\n",
    "\n",
    "# KNN \n",
    "# url: https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Decision Tree\n",
    "# url: https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Random Forest \n",
    "# url: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Logistic Classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.model_selection import learning_curve, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# PCA \n",
    "from sklearn.decomposition import PCA \n",
    "\n",
    "# Linear Regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_dep_score = pd.read_csv('../data/0&1/total_has_dep_diag.csv')\n",
    "\n",
    "X_t = total_dep_score.copy()\n",
    "del X_t['has_dep_diag']\n",
    "\n",
    "y_t = total_dep_score['has_dep_diag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2801\n",
       "1     242\n",
       "Name: has_dep_diag, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_t.value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9204732172198489"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2801 / (2801 + 242)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      comp_week  comp_wend  text_week  text_wend  tv_week  tv_wend\n",
      "0             2          2          1          1        2        2\n",
      "1             2          1          1          1        2        3\n",
      "2             3          3          3          1        2        2\n",
      "3             1          2          1          1        1        2\n",
      "4             2          3          0          0        2        2\n",
      "...         ...        ...        ...        ...      ...      ...\n",
      "3038          3          3          2          2        2        3\n",
      "3039          2          3          1          1        1        2\n",
      "3040          0          0          0          0        2        0\n",
      "3041          2          3          1          1        1        2\n",
      "3042          1          2          2          2        0        1\n",
      "\n",
      "[3043 rows x 6 columns]\n",
      "0       0\n",
      "1       0\n",
      "2       0\n",
      "3       0\n",
      "4       0\n",
      "       ..\n",
      "3038    0\n",
      "3039    0\n",
      "3040    0\n",
      "3041    1\n",
      "3042    0\n",
      "Name: has_dep_diag, Length: 3043, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(X_t)\n",
    "print(y_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_dep_score = pd.read_csv('../data/0&1/sampled_has_dep_diag.csv')\n",
    "\n",
    "X_s = sampled_dep_score.copy()\n",
    "del X_s['has_dep_diag']\n",
    "\n",
    "y_s = sampled_dep_score['has_dep_diag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    242\n",
       "1    242\n",
       "Name: has_dep_diag, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_s.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     comp_week  comp_wend  text_week  text_wend  tv_week  tv_wend\n",
      "0            2          2          1          1        3        3\n",
      "1            1          1          1          1        2        1\n",
      "2            1          2          3          3        2        2\n",
      "3            1          1          1          2        3        2\n",
      "4            1          1          1          1        2        2\n",
      "..         ...        ...        ...        ...      ...      ...\n",
      "479          2          3          0          0        3        3\n",
      "480          2          2          2          2        2        3\n",
      "481          1          2          1          1        1        2\n",
      "482          2          3          3          3        2        2\n",
      "483          2          2          1          1        1        2\n",
      "\n",
      "[484 rows x 6 columns]\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "479    1\n",
      "480    1\n",
      "481    1\n",
      "482    1\n",
      "483    0\n",
      "Name: has_dep_diag, Length: 484, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(X_s)\n",
    "print(y_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = KFold(n_splits = 5, shuffle = True, random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_t, y_t, test_size=0.3, random_state=1)  # 70% training and 30% test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_hyper_params = [ \n",
    "                        {\n",
    "                        'gamma': np.logspace(-4, -1, 4),\n",
    "                        'C': np.logspace(-3, 1, 5),\n",
    "                        'kernel': ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "                        }\n",
    "                    ]\n",
    "\n",
    "# specify model\n",
    "svm_model = svm.SVC(random_state=1)\n",
    "\n",
    "# set up GridSearchCV()\n",
    "svm_model_cv = GridSearchCV(estimator = svm_model, \n",
    "                            param_grid = svm_hyper_params, \n",
    "                            scoring= 'accuracy', \n",
    "                            cv = folds, \n",
    "                            verbose = 2,\n",
    "                            return_train_score=True,\n",
    "                            n_jobs=2)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 80 candidates, totalling 400 fits\n",
      "best hyper parameters {'C': 0.001, 'gamma': 0.0001, 'kernel': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "svm_model_cv.fit(X_train, y_train)\n",
    "print(\"best hyper parameters\", svm_model_cv.best_params_)\n",
    "svm_y_pred = svm_model_cv.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9200438116100766\n",
      "F1 score macro: 0.4791785510553337\n",
      "F1 score weighted: 0.8817305211094859\n"
     ]
    }
   ],
   "source": [
    "# accuracy \n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, svm_y_pred))\n",
    "# f1 score \n",
    "print(\"F1 score macro:\", f1_score(y_test, svm_y_pred, average='macro'))\n",
    "print(\"F1 score weighted:\", f1_score(y_test, svm_y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_hyper_params = [ \n",
    "                        {\n",
    "                        'C': np.logspace(-4, 2, 7),\n",
    "                        'solver' : ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "                        'penalty' : ['l1', 'l2', 'elasticnet', 'none'],\n",
    "                        'multi_class' : ['auto', 'ovr', 'multinomial']\n",
    "                        }\n",
    "                    ]\n",
    "\n",
    "# specify model\n",
    "log_model = LogisticRegression(random_state=1)\n",
    "\n",
    "# set up GridSearchCV()\n",
    "log_model_cv = GridSearchCV(estimator = log_model, \n",
    "                            param_grid = log_hyper_params, \n",
    "                            scoring= 'accuracy', \n",
    "                            cv = folds, \n",
    "                            verbose = 2,\n",
    "                            return_train_score=True,\n",
    "                            n_jobs=-1)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 420 candidates, totalling 2100 fits\n",
      "best hyper parameters {'C': 0.0001, 'multi_class': 'auto', 'penalty': 'l1', 'solver': 'liblinear'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sungjin/Documents/Programming/python/venv/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.92065728        nan 0.92065728 0.92065728\n",
      " 0.92065728 0.92065728 0.92065728 0.92065728        nan        nan\n",
      "        nan        nan        nan 0.92065728 0.92065728        nan\n",
      " 0.92065728 0.92065728        nan        nan 0.92065728        nan\n",
      " 0.92065728 0.92065728 0.92065728 0.92065728 0.92065728 0.92065728\n",
      "        nan        nan        nan        nan        nan 0.92065728\n",
      " 0.92065728        nan 0.92065728 0.92065728        nan        nan\n",
      "        nan        nan 0.92065728 0.92065728 0.92065728        nan\n",
      " 0.92065728 0.92065728        nan        nan        nan        nan\n",
      "        nan 0.92065728 0.92065728        nan 0.92065728 0.92065728\n",
      "        nan        nan 0.92065728        nan 0.92065728 0.92065728\n",
      " 0.92065728 0.92065728 0.92065728 0.92065728        nan        nan\n",
      "        nan        nan        nan 0.92065728 0.92065728        nan\n",
      " 0.92065728 0.92065728        nan        nan 0.92065728        nan\n",
      " 0.92065728 0.92065728 0.92065728 0.92065728 0.92065728 0.92065728\n",
      "        nan        nan        nan        nan        nan 0.92065728\n",
      " 0.92065728        nan 0.92065728 0.92065728        nan        nan\n",
      "        nan        nan 0.92065728 0.92065728 0.92065728        nan\n",
      " 0.92065728 0.92065728        nan        nan        nan        nan\n",
      "        nan 0.92065728 0.92065728        nan 0.92065728 0.92065728\n",
      "        nan        nan 0.92065728        nan 0.92065728 0.92065728\n",
      " 0.92065728 0.92065728 0.92065728 0.92065728        nan        nan\n",
      "        nan        nan        nan 0.92065728 0.92065728        nan\n",
      " 0.92065728 0.92065728        nan        nan 0.92065728        nan\n",
      " 0.92065728 0.92065728 0.92065728 0.92065728 0.92065728 0.92065728\n",
      "        nan        nan        nan        nan        nan 0.92065728\n",
      " 0.92065728        nan 0.92065728 0.92065728        nan        nan\n",
      "        nan        nan 0.92065728 0.92065728 0.92065728        nan\n",
      " 0.92065728 0.92065728        nan        nan        nan        nan\n",
      "        nan 0.92065728 0.92065728        nan 0.92065728 0.92065728\n",
      "        nan        nan 0.92065728        nan 0.92065728 0.92065728\n",
      " 0.92065728 0.92065728 0.92065728 0.92065728        nan        nan\n",
      "        nan        nan        nan 0.92065728 0.92065728        nan\n",
      " 0.92065728 0.92065728        nan        nan 0.92065728        nan\n",
      " 0.92065728 0.92065728 0.92065728 0.92065728 0.92065728 0.92065728\n",
      "        nan        nan        nan        nan        nan 0.92065728\n",
      " 0.92065728        nan 0.92065728 0.92065728        nan        nan\n",
      "        nan        nan 0.92065728 0.92065728 0.92065728        nan\n",
      " 0.92065728 0.92065728        nan        nan        nan        nan\n",
      "        nan 0.92065728 0.92065728        nan 0.92065728 0.92065728\n",
      "        nan        nan 0.92065728        nan 0.92065728 0.92065728\n",
      " 0.92065728 0.92065728 0.92065728 0.92065728        nan        nan\n",
      "        nan        nan        nan 0.92065728 0.92065728        nan\n",
      " 0.92065728 0.92065728        nan        nan 0.92065728        nan\n",
      " 0.92065728 0.92065728 0.92065728 0.92065728 0.92065728 0.92065728\n",
      "        nan        nan        nan        nan        nan 0.92065728\n",
      " 0.92065728        nan 0.92065728 0.92065728        nan        nan\n",
      "        nan        nan 0.92065728 0.92065728 0.92065728        nan\n",
      " 0.92065728 0.92065728        nan        nan        nan        nan\n",
      "        nan 0.92065728 0.92065728        nan 0.92065728 0.92065728\n",
      "        nan        nan 0.92065728        nan 0.92065728 0.92065728\n",
      " 0.92065728 0.92065728 0.92065728 0.92065728        nan        nan\n",
      "        nan        nan        nan 0.92065728 0.92065728        nan\n",
      " 0.92065728 0.92065728        nan        nan 0.92065728        nan\n",
      " 0.92065728 0.92065728 0.92065728 0.92065728 0.92065728 0.92065728\n",
      "        nan        nan        nan        nan        nan 0.92065728\n",
      " 0.92065728        nan 0.92065728 0.92065728        nan        nan\n",
      "        nan        nan 0.92065728 0.92065728 0.92065728        nan\n",
      " 0.92065728 0.92065728        nan        nan        nan        nan\n",
      "        nan 0.92065728 0.92065728        nan 0.92065728 0.92065728\n",
      "        nan        nan 0.92065728        nan 0.92065728 0.92065728\n",
      " 0.92065728 0.92065728 0.92065728 0.92065728        nan        nan\n",
      "        nan        nan        nan 0.92065728 0.92065728        nan\n",
      " 0.92065728 0.92065728        nan        nan 0.92065728        nan\n",
      " 0.92065728 0.92065728 0.92065728 0.92065728 0.92065728 0.92065728\n",
      "        nan        nan        nan        nan        nan 0.92065728\n",
      " 0.92065728        nan 0.92065728 0.92065728        nan        nan\n",
      "        nan        nan 0.92065728 0.92065728 0.92065728        nan\n",
      " 0.92065728 0.92065728        nan        nan        nan        nan\n",
      "        nan 0.92065728 0.92065728        nan 0.92065728 0.92065728]\n",
      "  warnings.warn(\n",
      "/home/sungjin/Documents/Programming/python/venv/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the train scores are non-finite: [       nan        nan 0.92065728        nan 0.92065728 0.92065728\n",
      " 0.92065728 0.92065728 0.92065728 0.92065728        nan        nan\n",
      "        nan        nan        nan 0.92065728 0.92065728        nan\n",
      " 0.92065728 0.92065728        nan        nan 0.92065728        nan\n",
      " 0.92065728 0.92065728 0.92065728 0.92065728 0.92065728 0.92065728\n",
      "        nan        nan        nan        nan        nan 0.92065728\n",
      " 0.92065728        nan 0.92065728 0.92065728        nan        nan\n",
      "        nan        nan 0.92065728 0.92065728 0.92065728        nan\n",
      " 0.92065728 0.92065728        nan        nan        nan        nan\n",
      "        nan 0.92065728 0.92065728        nan 0.92065728 0.92065728\n",
      "        nan        nan 0.92065728        nan 0.92065728 0.92065728\n",
      " 0.92065728 0.92065728 0.92065728 0.92065728        nan        nan\n",
      "        nan        nan        nan 0.92065728 0.92065728        nan\n",
      " 0.92065728 0.92065728        nan        nan 0.92065728        nan\n",
      " 0.92065728 0.92065728 0.92065728 0.92065728 0.92065728 0.92065728\n",
      "        nan        nan        nan        nan        nan 0.92065728\n",
      " 0.92065728        nan 0.92065728 0.92065728        nan        nan\n",
      "        nan        nan 0.92065728 0.92065728 0.92065728        nan\n",
      " 0.92065728 0.92065728        nan        nan        nan        nan\n",
      "        nan 0.92065728 0.92065728        nan 0.92065728 0.92065728\n",
      "        nan        nan 0.92065728        nan 0.92065728 0.92065728\n",
      " 0.92065728 0.92065728 0.92065728 0.92065728        nan        nan\n",
      "        nan        nan        nan 0.92065728 0.92065728        nan\n",
      " 0.92065728 0.92065728        nan        nan 0.92065728        nan\n",
      " 0.92065728 0.92065728 0.92065728 0.92065728 0.92065728 0.92065728\n",
      "        nan        nan        nan        nan        nan 0.92065728\n",
      " 0.92065728        nan 0.92065728 0.92065728        nan        nan\n",
      "        nan        nan 0.92065728 0.92065728 0.92065728        nan\n",
      " 0.92065728 0.92065728        nan        nan        nan        nan\n",
      "        nan 0.92065728 0.92065728        nan 0.92065728 0.92065728\n",
      "        nan        nan 0.92065728        nan 0.92065728 0.92065728\n",
      " 0.92065728 0.92065728 0.92065728 0.92065728        nan        nan\n",
      "        nan        nan        nan 0.92065728 0.92065728        nan\n",
      " 0.92065728 0.92065728        nan        nan 0.92065728        nan\n",
      " 0.92065728 0.92065728 0.92065728 0.92065728 0.92065728 0.92065728\n",
      "        nan        nan        nan        nan        nan 0.92065728\n",
      " 0.92065728        nan 0.92065728 0.92065728        nan        nan\n",
      "        nan        nan 0.92065728 0.92065728 0.92065728        nan\n",
      " 0.92065728 0.92065728        nan        nan        nan        nan\n",
      "        nan 0.92065728 0.92065728        nan 0.92065728 0.92065728\n",
      "        nan        nan 0.92065728        nan 0.92065728 0.92065728\n",
      " 0.92065728 0.92065728 0.92065728 0.92065728        nan        nan\n",
      "        nan        nan        nan 0.92065728 0.92065728        nan\n",
      " 0.92065728 0.92065728        nan        nan 0.92065728        nan\n",
      " 0.92065728 0.92065728 0.92065728 0.92065728 0.92065728 0.92065728\n",
      "        nan        nan        nan        nan        nan 0.92065728\n",
      " 0.92065728        nan 0.92065728 0.92065728        nan        nan\n",
      "        nan        nan 0.92065728 0.92065728 0.92065728        nan\n",
      " 0.92065728 0.92065728        nan        nan        nan        nan\n",
      "        nan 0.92065728 0.92065728        nan 0.92065728 0.92065728\n",
      "        nan        nan 0.92065728        nan 0.92065728 0.92065728\n",
      " 0.92065728 0.92065728 0.92065728 0.92065728        nan        nan\n",
      "        nan        nan        nan 0.92065728 0.92065728        nan\n",
      " 0.92065728 0.92065728        nan        nan 0.92065728        nan\n",
      " 0.92065728 0.92065728 0.92065728 0.92065728 0.92065728 0.92065728\n",
      "        nan        nan        nan        nan        nan 0.92065728\n",
      " 0.92065728        nan 0.92065728 0.92065728        nan        nan\n",
      "        nan        nan 0.92065728 0.92065728 0.92065728        nan\n",
      " 0.92065728 0.92065728        nan        nan        nan        nan\n",
      "        nan 0.92065728 0.92065728        nan 0.92065728 0.92065728\n",
      "        nan        nan 0.92065728        nan 0.92065728 0.92065728\n",
      " 0.92065728 0.92065728 0.92065728 0.92065728        nan        nan\n",
      "        nan        nan        nan 0.92065728 0.92065728        nan\n",
      " 0.92065728 0.92065728        nan        nan 0.92065728        nan\n",
      " 0.92065728 0.92065728 0.92065728 0.92065728 0.92065728 0.92065728\n",
      "        nan        nan        nan        nan        nan 0.92065728\n",
      " 0.92065728        nan 0.92065728 0.92065728        nan        nan\n",
      "        nan        nan 0.92065728 0.92065728 0.92065728        nan\n",
      " 0.92065728 0.92065728        nan        nan        nan        nan\n",
      "        nan 0.92065728 0.92065728        nan 0.92065728 0.92065728]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "log_model_cv.fit(X_train, y_train)\n",
    "print(\"best hyper parameters\", log_model_cv.best_params_)\n",
    "log_y_pred = log_model_cv.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9200438116100766\n",
      "F1 score macro: 0.4791785510553337\n",
      "F1 score weighted: 0.8817305211094859\n"
     ]
    }
   ],
   "source": [
    "# accuracy \n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, log_y_pred))\n",
    "# f1 score \n",
    "print(\"F1 score macro:\", f1_score(y_test, log_y_pred, average='macro'))\n",
    "print(\"F1 score weighted:\", f1_score(y_test, log_y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_hyper_params = [ \n",
    "                        {\n",
    "                        'weights' : ['uniform', 'distance'],\n",
    "                        'algorithm' : ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "                        'leaf_size' : np.linspace(2, 100, 10, dtype=int)\n",
    "                        }\n",
    "                    ]\n",
    "\n",
    "# specify model\n",
    "\n",
    "# THIS SECTION SHOULD BE CHANGED.\n",
    "# n_neighbors  SHOULD BE MODIFIED TO ANOTHER VALUE DEPENDING ON THE TARGET VALUE.\n",
    "knn_model = KNeighborsClassifier(n_neighbors=len(y_t.unique()))\n",
    "\n",
    "# set up GridSearchCV()\n",
    "knn_model_cv = GridSearchCV(estimator = knn_model, \n",
    "                            param_grid = knn_hyper_params, \n",
    "                            scoring= 'accuracy', \n",
    "                            cv = folds, \n",
    "                            verbose = 2,\n",
    "                            return_train_score=True,\n",
    "                            n_jobs=-1)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 80 candidates, totalling 400 fits\n",
      "best hyper parameters {'algorithm': 'auto', 'leaf_size': 23, 'weights': 'uniform'}\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "knn_model_cv.fit(X_train, y_train)\n",
    "print(\"best hyper parameters\", knn_model_cv.best_params_)\n",
    "knn_y_pred = knn_model_cv.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.91894852135816\n",
      "F1 score macro: 0.47888127853881274\n",
      "F1 score weighted: 0.8811835136311121\n"
     ]
    }
   ],
   "source": [
    "# accuracy \n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, knn_y_pred))\n",
    "# f1 score \n",
    "print(\"F1 score macro:\", f1_score(y_test, knn_y_pred, average='macro'))\n",
    "print(\"F1 score weighted:\", f1_score(y_test, knn_y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_hyper_params = [ \n",
    "                        {\n",
    "                            'n_estimators' : [int(x) for x in np.linspace(5, 50, 5)],\n",
    "                            'criterion' : ['gini', 'entropy'],\n",
    "                            'max_depth' : [int(x) for x in np.linspace(2, 50, 5)],\n",
    "                            'min_samples_split' : [int(x) for x in np.linspace(2, 50, 5)],\n",
    "                            'min_samples_leaf' : [int(x) for x in np.linspace(2, 50, 5)],\n",
    "                            'max_features' : ['auto', 'sqrt', 'log2'],\n",
    "                            'bootstrap' : [True, False]\n",
    "\n",
    "                        }\n",
    "                    ]\n",
    "\n",
    "# specify model\n",
    "\n",
    "# THIS SECTION SHOULD BE CHANGED.\n",
    "# n_neighbors  SHOULD BE MODIFIED TO ANOTHER VALUE DEPENDING ON THE TARGET VALUE.\n",
    "rf_model = RandomForestClassifier(random_state=1)\n",
    "\n",
    "# set up GridSearchCV()\n",
    "rf_model_cv = GridSearchCV(estimator = rf_model, \n",
    "                            param_grid = rf_hyper_params, \n",
    "                            scoring= 'accuracy', \n",
    "                            cv = folds, \n",
    "                            verbose = 2,\n",
    "                            return_train_score=True,\n",
    "                            n_jobs=-1)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 7500 candidates, totalling 37500 fits\n",
      "best hyper parameters {'bootstrap': False, 'criterion': 'gini', 'max_depth': 14, 'max_features': 'auto', 'min_samples_leaf': 2, 'min_samples_split': 26, 'n_estimators': 16}\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "rf_model_cv.fit(X_train, y_train)\n",
    "print(\"best hyper parameters\", rf_model_cv.best_params_)\n",
    "rf_y_pred = rf_model_cv.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.91894852135816\n",
      "F1 score macro: 0.47888127853881274\n",
      "F1 score weighted: 0.8811835136311121\n"
     ]
    }
   ],
   "source": [
    "# accuracy \n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, rf_y_pred))\n",
    "# f1 score \n",
    "print(\"F1 score macro:\", f1_score(y_test, rf_y_pred, average='macro'))\n",
    "print(\"F1 score weighted:\", f1_score(y_test, rf_y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_s, y_s, test_size=0.3, random_state=1)  # 70% training and 30% test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_hyper_params = [ \n",
    "                        {\n",
    "                        'gamma': np.logspace(-4, -1, 4),\n",
    "                        'C': np.logspace(-3, 1, 5),\n",
    "                        'kernel': ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "                        }\n",
    "                    ]\n",
    "\n",
    "# specify model\n",
    "svm_model = svm.SVC(random_state=1)\n",
    "\n",
    "# set up GridSearchCV()\n",
    "svm_model_cv = GridSearchCV(estimator = svm_model, \n",
    "                            param_grid = svm_hyper_params, \n",
    "                            scoring= 'accuracy', \n",
    "                            cv = folds, \n",
    "                            verbose = 2,\n",
    "                            return_train_score=True,\n",
    "                            n_jobs=2)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 80 candidates, totalling 400 fits\n",
      "best hyper parameters {'C': 0.001, 'gamma': 0.0001, 'kernel': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "svm_model_cv.fit(X_train, y_train)\n",
    "print(\"best hyper parameters\", svm_model_cv.best_params_)\n",
    "svm_y_pred = svm_model_cv.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4657534246575342\n",
      "F1 score macro: 0.3177570093457944\n",
      "F1 score weighted: 0.2959928306234797\n"
     ]
    }
   ],
   "source": [
    "# accuracy \n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, svm_y_pred))\n",
    "# f1 score \n",
    "print(\"F1 score macro:\", f1_score(y_test, svm_y_pred, average='macro'))\n",
    "print(\"F1 score weighted:\", f1_score(y_test, svm_y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_hyper_params = [ \n",
    "                        {\n",
    "                        'C': np.logspace(-4, 2, 7),\n",
    "                        'solver' : ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "                        'penalty' : ['l1', 'l2', 'elasticnet', 'none'],\n",
    "                        'multi_class' : ['auto', 'ovr', 'multinomial']\n",
    "                        }\n",
    "                    ]\n",
    "\n",
    "# specify model\n",
    "log_model = LogisticRegression(random_state=1)\n",
    "\n",
    "# set up GridSearchCV()\n",
    "log_model_cv = GridSearchCV(estimator = log_model, \n",
    "                            param_grid = log_hyper_params, \n",
    "                            scoring= 'accuracy', \n",
    "                            cv = folds, \n",
    "                            verbose = 2,\n",
    "                            return_train_score=True,\n",
    "                            n_jobs=-1)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 420 candidates, totalling 2100 fits\n",
      "best hyper parameters {'C': 0.0001, 'multi_class': 'multinomial', 'penalty': 'l1', 'solver': 'saga'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sungjin/Documents/Programming/python/venv/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.48516242        nan 0.51483758 0.51483758\n",
      " 0.51483758 0.4761633  0.51483758 0.51483758        nan        nan\n",
      "        nan        nan        nan 0.45851624 0.45851624        nan\n",
      " 0.45851624 0.45851624        nan        nan 0.48516242        nan\n",
      " 0.51483758 0.51483758 0.51483758 0.4761633  0.51483758 0.51483758\n",
      "        nan        nan        nan        nan        nan 0.45851624\n",
      " 0.45851624        nan 0.45851624 0.45851624        nan        nan\n",
      "        nan        nan 0.52071993 0.51483758 0.51483758        nan\n",
      " 0.51483758 0.51483758        nan        nan        nan        nan\n",
      "        nan 0.45851624 0.45851624        nan 0.45851624 0.45851624\n",
      "        nan        nan 0.48516242        nan 0.51483758 0.50588235\n",
      " 0.50588235 0.47923617 0.50588235 0.50588235        nan        nan\n",
      "        nan        nan        nan 0.45851624 0.45851624        nan\n",
      " 0.45851624 0.45851624        nan        nan 0.48516242        nan\n",
      " 0.51483758 0.50588235 0.50588235 0.47923617 0.50588235 0.50588235\n",
      "        nan        nan        nan        nan        nan 0.45851624\n",
      " 0.45851624        nan 0.45851624 0.45851624        nan        nan\n",
      "        nan        nan 0.52071993 0.49697103 0.49697103        nan\n",
      " 0.49697103 0.49697103        nan        nan        nan        nan\n",
      "        nan 0.45851624 0.45851624        nan 0.45851624 0.45851624\n",
      "        nan        nan 0.48516242        nan 0.51483758 0.49705882\n",
      " 0.49705882 0.48525022 0.49705882 0.49705882        nan        nan\n",
      "        nan        nan        nan 0.45851624 0.45851624        nan\n",
      " 0.45851624 0.45851624        nan        nan 0.48516242        nan\n",
      " 0.51483758 0.49705882 0.49705882 0.48525022 0.49705882 0.49705882\n",
      "        nan        nan        nan        nan        nan 0.45851624\n",
      " 0.45851624        nan 0.45851624 0.45851624        nan        nan\n",
      "        nan        nan 0.51483758 0.5000439  0.5000439         nan\n",
      " 0.5000439  0.5000439         nan        nan        nan        nan\n",
      "        nan 0.45851624 0.45851624        nan 0.45851624 0.45851624\n",
      "        nan        nan 0.46150132        nan 0.51483758 0.47335382\n",
      " 0.47335382 0.44964881 0.47335382 0.47335382        nan        nan\n",
      "        nan        nan        nan 0.45851624 0.45851624        nan\n",
      " 0.45851624 0.45851624        nan        nan 0.46150132        nan\n",
      " 0.51483758 0.47335382 0.47335382 0.44964881 0.47335382 0.47335382\n",
      "        nan        nan        nan        nan        nan 0.45851624\n",
      " 0.45851624        nan 0.45851624 0.45851624        nan        nan\n",
      "        nan        nan 0.51483758 0.48213345 0.48213345        nan\n",
      " 0.48213345 0.48213345        nan        nan        nan        nan\n",
      "        nan 0.45851624 0.45851624        nan 0.45851624 0.45851624\n",
      "        nan        nan 0.45860404        nan 0.47928007 0.46444249\n",
      " 0.46444249 0.4761633  0.46444249 0.46742757        nan        nan\n",
      "        nan        nan        nan 0.45851624 0.45851624        nan\n",
      " 0.45851624 0.45851624        nan        nan 0.45860404        nan\n",
      " 0.47928007 0.46444249 0.46444249 0.4761633  0.46444249 0.46742757\n",
      "        nan        nan        nan        nan        nan 0.45851624\n",
      " 0.45851624        nan 0.45851624 0.45851624        nan        nan\n",
      "        nan        nan 0.47928007 0.45851624 0.45851624        nan\n",
      " 0.45851624 0.45851624        nan        nan        nan        nan\n",
      "        nan 0.45851624 0.45851624        nan 0.45851624 0.45851624\n",
      "        nan        nan 0.4643986         nan 0.46145742 0.45851624\n",
      " 0.45851624 0.45851624 0.45851624 0.45851624        nan        nan\n",
      "        nan        nan        nan 0.45851624 0.45851624        nan\n",
      " 0.45851624 0.45851624        nan        nan 0.4643986         nan\n",
      " 0.46145742 0.45851624 0.45851624 0.45851624 0.45851624 0.45851624\n",
      "        nan        nan        nan        nan        nan 0.45851624\n",
      " 0.45851624        nan 0.45851624 0.45851624        nan        nan\n",
      "        nan        nan 0.46444249 0.45851624 0.45851624        nan\n",
      " 0.45851624 0.45851624        nan        nan        nan        nan\n",
      "        nan 0.45851624 0.45851624        nan 0.45851624 0.45851624\n",
      "        nan        nan 0.45851624        nan 0.45851624 0.45851624\n",
      " 0.45851624 0.45851624 0.45851624 0.45851624        nan        nan\n",
      "        nan        nan        nan 0.45851624 0.45851624        nan\n",
      " 0.45851624 0.45851624        nan        nan 0.45851624        nan\n",
      " 0.45851624 0.45851624 0.45851624 0.45851624 0.45851624 0.45851624\n",
      "        nan        nan        nan        nan        nan 0.45851624\n",
      " 0.45851624        nan 0.45851624 0.45851624        nan        nan\n",
      "        nan        nan 0.45851624 0.45851624 0.45851624        nan\n",
      " 0.45851624 0.45851624        nan        nan        nan        nan\n",
      "        nan 0.45851624 0.45851624        nan 0.45851624 0.45851624]\n",
      "  warnings.warn(\n",
      "/home/sungjin/Documents/Programming/python/venv/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the train scores are non-finite: [       nan        nan 0.48520432        nan 0.51479568 0.51479568\n",
      " 0.51479568 0.51624983 0.51479568 0.51479568        nan        nan\n",
      "        nan        nan        nan 0.53032664 0.53032664        nan\n",
      " 0.53032664 0.53032664        nan        nan 0.48520432        nan\n",
      " 0.51479568 0.51479568 0.51479568 0.51624983 0.51479568 0.51479568\n",
      "        nan        nan        nan        nan        nan 0.53032664\n",
      " 0.53032664        nan 0.53032664 0.53032664        nan        nan\n",
      "        nan        nan 0.50590679 0.51479568 0.51479568        nan\n",
      " 0.51479568 0.51479568        nan        nan        nan        nan\n",
      "        nan 0.53032664 0.53032664        nan 0.53032664 0.53032664\n",
      "        nan        nan 0.48520432        nan 0.51479568 0.51922373\n",
      " 0.51922373 0.5162799  0.51922373 0.51922373        nan        nan\n",
      "        nan        nan        nan 0.53032664 0.53032664        nan\n",
      " 0.53032664 0.53032664        nan        nan 0.48520432        nan\n",
      " 0.51479568 0.51922373 0.51922373 0.5162799  0.51922373 0.51922373\n",
      "        nan        nan        nan        nan        nan 0.53032664\n",
      " 0.53032664        nan 0.53032664 0.53032664        nan        nan\n",
      "        nan        nan 0.50590679 0.51923193 0.51923193        nan\n",
      " 0.51923193 0.51923193        nan        nan        nan        nan\n",
      "        nan 0.53032664 0.53032664        nan 0.53032664 0.53032664\n",
      "        nan        nan 0.48520432        nan 0.51479568 0.51626896\n",
      " 0.51626896 0.51847752 0.51626896 0.51626896        nan        nan\n",
      "        nan        nan        nan 0.53032664 0.53032664        nan\n",
      " 0.53032664 0.53032664        nan        nan 0.48520432        nan\n",
      " 0.51479568 0.51626896 0.51626896 0.51847752 0.51626896 0.51626896\n",
      "        nan        nan        nan        nan        nan 0.53032664\n",
      " 0.53032664        nan 0.53032664 0.53032664        nan        nan\n",
      "        nan        nan 0.51479568 0.52217576 0.52217576        nan\n",
      " 0.52217576 0.52217576        nan        nan        nan        nan\n",
      "        nan 0.53032664 0.53032664        nan 0.53032664 0.53032664\n",
      "        nan        nan 0.50887522        nan 0.51479568 0.52809895\n",
      " 0.52809895 0.52219216 0.52809895 0.52809895        nan        nan\n",
      "        nan        nan        nan 0.53032664 0.53032664        nan\n",
      " 0.53032664 0.53032664        nan        nan 0.50887522        nan\n",
      " 0.51479568 0.52809895 0.52809895 0.52219216 0.52809895 0.52809895\n",
      "        nan        nan        nan        nan        nan 0.53032664\n",
      " 0.53032664        nan 0.53032664 0.53032664        nan        nan\n",
      "        nan        nan 0.51479568 0.51996173 0.51996173        nan\n",
      " 0.51996173 0.51996173        nan        nan        nan        nan\n",
      "        nan 0.53032664 0.53032664        nan 0.53032664 0.53032664\n",
      "        nan        nan 0.52440344        nan 0.52514692 0.52884516\n",
      " 0.52884516 0.52885062 0.52884516 0.52884516        nan        nan\n",
      "        nan        nan        nan 0.53032664 0.53032664        nan\n",
      " 0.53032664 0.53032664        nan        nan 0.52440344        nan\n",
      " 0.52514692 0.52884516 0.52884516 0.52885062 0.52884516 0.52884516\n",
      "        nan        nan        nan        nan        nan 0.53032664\n",
      " 0.53032664        nan 0.53032664 0.53032664        nan        nan\n",
      "        nan        nan 0.52588766 0.52958863 0.52958863        nan\n",
      " 0.52958863 0.52958863        nan        nan        nan        nan\n",
      "        nan 0.53032664 0.53032664        nan 0.53032664 0.53032664\n",
      "        nan        nan 0.53254886        nan 0.52810715 0.53032664\n",
      " 0.53032664 0.53032664 0.53032664 0.53032664        nan        nan\n",
      "        nan        nan        nan 0.53032664 0.53032664        nan\n",
      " 0.53032664 0.53032664        nan        nan 0.53254886        nan\n",
      " 0.52810715 0.53032664 0.53032664 0.53032664 0.53032664 0.53032664\n",
      "        nan        nan        nan        nan        nan 0.53032664\n",
      " 0.53032664        nan 0.53032664 0.53032664        nan        nan\n",
      "        nan        nan 0.52810715 0.53032664 0.53032664        nan\n",
      " 0.53032664 0.53032664        nan        nan        nan        nan\n",
      "        nan 0.53032664 0.53032664        nan 0.53032664 0.53032664\n",
      "        nan        nan 0.53032664        nan 0.53032664 0.53032664\n",
      " 0.53032664 0.53032664 0.53032664 0.53032664        nan        nan\n",
      "        nan        nan        nan 0.53032664 0.53032664        nan\n",
      " 0.53032664 0.53032664        nan        nan 0.53032664        nan\n",
      " 0.53032664 0.53032664 0.53032664 0.53032664 0.53032664 0.53032664\n",
      "        nan        nan        nan        nan        nan 0.53032664\n",
      " 0.53032664        nan 0.53032664 0.53032664        nan        nan\n",
      "        nan        nan 0.53032664 0.53032664 0.53032664        nan\n",
      " 0.53032664 0.53032664        nan        nan        nan        nan\n",
      "        nan 0.53032664 0.53032664        nan 0.53032664 0.53032664]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "log_model_cv.fit(X_train, y_train)\n",
    "print(\"best hyper parameters\", log_model_cv.best_params_)\n",
    "log_y_pred = log_model_cv.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5342465753424658\n",
      "F1 score macro: 0.34821428571428575\n",
      "F1 score weighted: 0.3720645792563601\n"
     ]
    }
   ],
   "source": [
    "# accuracy \n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, log_y_pred))\n",
    "# f1 score \n",
    "print(\"F1 score macro:\", f1_score(y_test, log_y_pred, average='macro'))\n",
    "print(\"F1 score weighted:\", f1_score(y_test, log_y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_hyper_params = [ \n",
    "                        {\n",
    "                        'weights' : ['uniform', 'distance'],\n",
    "                        'algorithm' : ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "                        'leaf_size' : np.linspace(2, 100, 10, dtype=int)\n",
    "                        }\n",
    "                    ]\n",
    "\n",
    "# specify model\n",
    "\n",
    "# THIS SECTION SHOULD BE CHANGED.\n",
    "# n_neighbors  SHOULD BE MODIFIED TO ANOTHER VALUE DEPENDING ON THE TARGET VALUE.\n",
    "knn_model = KNeighborsClassifier(n_neighbors=len(y_t.unique()))\n",
    "\n",
    "# set up GridSearchCV()\n",
    "knn_model_cv = GridSearchCV(estimator = knn_model, \n",
    "                            param_grid = knn_hyper_params, \n",
    "                            scoring= 'accuracy', \n",
    "                            cv = folds, \n",
    "                            verbose = 2,\n",
    "                            return_train_score=True,\n",
    "                            n_jobs=-1)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 80 candidates, totalling 400 fits\n",
      "best hyper parameters {'algorithm': 'ball_tree', 'leaf_size': 2, 'weights': 'distance'}\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "knn_model_cv.fit(X_train, y_train)\n",
    "print(\"best hyper parameters\", knn_model_cv.best_params_)\n",
    "knn_y_pred = knn_model_cv.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5\n",
      "F1 score macro: 0.47308053591733823\n",
      "F1 score weighted: 0.4812379492757206\n"
     ]
    }
   ],
   "source": [
    "# accuracy \n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, knn_y_pred))\n",
    "# f1 score \n",
    "print(\"F1 score macro:\", f1_score(y_test, knn_y_pred, average='macro'))\n",
    "print(\"F1 score weighted:\", f1_score(y_test, knn_y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_hyper_params = [ \n",
    "                        {\n",
    "                            'n_estimators' : [int(x) for x in np.linspace(5, 50, 5)],\n",
    "                            'criterion' : ['gini', 'entropy'],\n",
    "                            'max_depth' : [int(x) for x in np.linspace(2, 50, 5)],\n",
    "                            'min_samples_split' : [int(x) for x in np.linspace(2, 50, 5)],\n",
    "                            'min_samples_leaf' : [int(x) for x in np.linspace(2, 50, 5)],\n",
    "                            'max_features' : ['auto', 'sqrt', 'log2'],\n",
    "                            'bootstrap' : [True, False]\n",
    "\n",
    "                        }\n",
    "                    ]\n",
    "\n",
    "# specify model\n",
    "\n",
    "# THIS SECTION SHOULD BE CHANGED.\n",
    "# n_neighbors  SHOULD BE MODIFIED TO ANOTHER VALUE DEPENDING ON THE TARGET VALUE.\n",
    "rf_model = RandomForestClassifier(random_state=1)\n",
    "\n",
    "# set up GridSearchCV()\n",
    "rf_model_cv = GridSearchCV(estimator = rf_model, \n",
    "                            param_grid = rf_hyper_params, \n",
    "                            scoring= 'accuracy', \n",
    "                            cv = folds, \n",
    "                            verbose = 2,\n",
    "                            return_train_score=True,\n",
    "                            n_jobs=-1)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 7500 candidates, totalling 37500 fits\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "rf_model_cv.fit(X_train, y_train)\n",
    "print(\"best hyper parameters\", rf_model_cv.best_params_)\n",
    "rf_y_pred = rf_model_cv.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5947368421052631\n",
      "F1 score macro: 0.4065149486836233\n",
      "F1 score weighted: 0.4733601070950468\n"
     ]
    }
   ],
   "source": [
    "# accuracy \n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, rf_y_pred))\n",
    "# f1 score \n",
    "print(\"F1 score macro:\", f1_score(y_test, rf_y_pred, average='macro'))\n",
    "print(\"F1 score weighted:\", f1_score(y_test, rf_y_pred, average='weighted'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
